{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] =\n",
        "# Initialize Hugging Face transformer model\n",
        "model_name = \"gpt2\"  # You can replace with other smaller open-source models from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Initialize LangChain with Hugging Face, providing the model_name as repo_id\n",
        "# llm = HuggingFaceHub(model=model, tokenizer=tokenizer)  # This line is replaced\n",
        "llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.7, \"max_new_tokens\":512}) # provide repo_id instead of the model and tokenizer\n",
        "\n",
        "# Define the Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"\"\"\n",
        "    Create an outline for a PowerPoint presentation on the topic '{topic}'.\n",
        "    The outline should include the title and key points for each slide.\n",
        "    Aim for 5-6 slides with concise, relevant information.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Create an LLMChain for slide generation\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Function to generate slide content using the LLM\n",
        "def generate_slide_content(topic):\n",
        "    # Run the chain to get a response\n",
        "    response = chain.run({\"topic\": topic})\n",
        "    slides_content = response.split(\"\\n\\n\")  # Assume each slide's content is separated by double newlines\n",
        "    return slides_content\n",
        "\n",
        "# Function to create a PowerPoint presentation\n",
        "def create_ppt(slide_contents, output_filename=\"AI_Presentation.pptx\"):\n",
        "    # Initialize a PowerPoint Presentation\n",
        "    prs = Presentation()\n",
        "\n",
        "    # Iterate over slide contents\n",
        "    for content in slide_contents:\n",
        "        slide = prs.slides.add_slide(prs.slide_layouts[1])  # Use a layout with title and content\n",
        "\n",
        "        # Split content by title and body\n",
        "        lines = content.split(\"\\n\")\n",
        "        title = lines[0] if lines else \"Title\"\n",
        "        body = \"\\n\".join(lines[1:])\n",
        "\n",
        "        # Set slide title\n",
        "        slide.shapes.title.text = title\n",
        "\n",
        "        # Add slide content\n",
        "        text_box = slide.shapes.add_textbox(Inches(1), Inches(1.5), Inches(8), Inches(4))\n",
        "        tf = text_box.text_frame\n",
        "        p = tf.add_paragraph()\n",
        "        p.text = body\n",
        "        p.font.size = Pt(18)\n",
        "        p.alignment = PP_ALIGN.LEFT\n",
        "\n",
        "    # Save the PowerPoint presentation\n",
        "    prs.save(output_filename)\n",
        "    print(f\"Presentation saved as {output_filename}\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    topic = \"Machine Learning Basics\"  # Example topic\n",
        "    slide_contents = generate_slide_content(topic)  # Get slide content from LLM\n",
        "    create_ppt(slide_contents, \"Machine_Learning_Presentation.pptx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe_3Quy1NOJj",
        "outputId": "fd45a6db-d0dc-4839-8560-6125ab7648fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-10-e8765639d988>:31: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n",
            "<ipython-input-10-e8765639d988>:36: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run({\"topic\": topic})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation saved as Machine_Learning_Presentation.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ONcV72o3SxdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}